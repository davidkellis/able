# `able test` CLI Design (Draft)

## Goals

- Provide a low-ceremony command that discovers tests, applies filters, runs frameworks via the shared protocol, and reports results with consistent exit codes.
- Keep all interpreter/runtime specifics inside the stdlib harness; the CLI orchestrates discovery and reporting.
- Offer structured outputs suitable for automation (doc/progress/TAP/JSON).
- Avoid hidden imports; test files explicitly import frameworks, and the CLI simply loads the modules that already exist.

## Non-Goals (Separation of Concerns)

- `able test` is a **user-facing** testing workflow. It does not cover language
  implementation fixtures, parity harnesses, or spec conformance checks.
- Fixture suites remain the canonical mechanism for interpreter/spec validation
  and are intentionally separate from `able test`.

## Command Surface

```text
able test [PATHS...]

Options:
  --path <PATTERN>          Include tests whose module path contains PATTERN (repeatable)
  --exclude-path <PATTERN>  Exclude paths containing PATTERN
  --name <SUBSTRING>        Include tests whose ID/display name contains substring
  --exclude-name <SUBSTRING>
  --tag <TAG>               Include tests with TAG (repeatable)
  --exclude-tag <TAG>
  --list                    Discover, print, and exit (no execution)
  --format <doc|progress|tap|json> (default: doc)
  --shuffle [SEED]          Shuffle execution order; optional i64 seed for repeatability
  --fail-fast               Abort on first failure
  --parallel <N>            Desired max parallelism (framework honours when possible)
  --repeat <COUNT>          Repeat entire plan COUNT times (default 1)
  --dry-run                 Load modules and perform discovery but skip execution (implies --list with metadata)
  --filter-file <PATH>      Optional configuration file for filters (future)
  --help                    Show usage
```

- Paths restrict module loading scope. Without path arguments, the CLI scans the workspace for `.test.able` / `.spec.able` files under known packages.
  - Implementation note: reuse the package loader’s knowledge of package roots to avoid double logic.
- Tag/name filters align with the framework metadata generated by spec-style suites.

## High-Level Flow

1. **Prepare Environment**
   - Parse CLI flags; validate conflicting options (e.g., `--list` with `--repeat` should warn but still run discovery).

2. **Module Loading**
   - Determine module list using suffix convention. For each candidate file:
     - Load/execute module using interpreter (Bun/Go runtime). Module evaluation should trigger framework registration via imports.
     - Record load errors; if any module fails to load, emit diagnostic and exit code `2`.

3. **Discovery**
   - Construct `DiscoveryRequest` from CLI filters.
   - Call `able.test.harness.discover_all(request)`.
     - If it returns a `Failure`, print diagnostic (include `message`, optional `details`) and exit `2`.
   - Receive descriptors array. If empty, exit `0` unless `--fail-fast`/`--repeat` make emptiness a warning (print “no tests found”).

4. **List Mode**
   - When `--list` or `--dry-run`, iterate descriptors and print:
     - `framework_id | suite_path | test_id | display_name | tags | module_path`
     - For JSON output, serialize descriptors directly.
   - Skip execution; exit `0`.

5. **Reporter Selection**
   - Map `--format` to reporter:
     - `doc` → `able.test.reporters.DocReporter` writing to stdout.
     - `progress` → `able.test.reporters.ProgressReporter`, flush on finish.
     - `tap` → future TAP reporter (not yet implemented).
     - `json` → reporter that streams JSON events (should wrap `Reporter` interface).
   - Provide optional `--quiet`/`--color` overrides later if needed.

6. **Run Options**
   - Build `RunOptions { shuffle_seed, fail_fast, parallelism, repeat }`.
     - `shuffle_seed`: when user provides `--shuffle` without seed, generate one (e.g., using RNG) and display it for reproducibility.
     - Honour `--repeat` by executing harness run multiple times; frameworks themselves need not handle repetition.

7. **Execution**
   - Call `able.test.harness.run_plan` with `TestPlan { descriptors }`, run options, reporter.
   - For each reporter event:
     - For doc/progress, write to stdout immediately.
     - For TAP/JSON, convert to format-specific output.
   - Track whether any `case_failed` events occurred to influence exit code.
   - If harness returns `Failure`, treat as execution error:
     - Print message to stderr (`framework error: ...`).
     - Exit code `2`.

8. **Exit Codes**
   - `0`: no failures/errors.
   - `1`: at least one test failed (`case_failed` event without update mode).
   - `2`: discovery/loader/framework error (`Failure` from harness or module load failure).
   - Higher codes reserved for CLI/system errors.

9. **Cleanup**
   - Flush progress reporter buffer (`finish()`).
   - Optionally print run summary (`N passed, M failed, duration ...`).

## Responsibilities Split

### CLI

- Command-line parsing, validation, and help output.
- Finding test files/module paths.
- Loading modules through interpreter entry points.
- Constructing `DiscoveryRequest`, `RunOptions`.
- Reporter instantiation and mapping to CLI format options.
- Summary output.
- Exit code determination.

### Stdlib (Already Implemented)

- Framework registration and discovery/run logic per framework.
- Expectation matchers.
- Reporter implementations.
- Harness `discover_all`, `run_plan`, `run_all`.

## TAP/JSON Reporters (Future)

- TAP reporter: produce `ok`/`not ok` lines referencing descriptor IDs; embed failure details in YAML.
- JSON reporter: emit newline-delimited JSON for each event (`{ event: "case_passed", descriptor: {...}, duration_ms: ... }`).
- Both should wrap the `Reporter` interface and may require richer metadata from matchers (already available via `details`).

## Open Questions

- Should `--pattern` (glob) be supported in addition to `--path` substring? Might rely on `globset` port later.
- How should parallelism be implemented in the CLI layer vs. framework? For now, CLI can divide descriptors per framework and run frameworks sequentially, leaving concurrency to frameworks.
- Do we need per-test timeouts? Could be added to `RunOptions` later.
- Should build artifacts (e.g., compiled modules) be incremental between runs? Reuse interpreter caching rather than CLI.

## Next Steps

- Decide flag names/short aliases and final output formatting.
- Draft reporter implementations for TAP/JSON.
- Implement CLI command following this spec once parser/loader dependencies are ready.
